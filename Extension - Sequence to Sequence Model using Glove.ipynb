{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attempt is made to implement Sequence to Sequence (encoder -decoder) model to predict title for the paragraph passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading Libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Dense, Input\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up directories\n",
    "\n",
    "np.random.seed(42)\n",
    "data_dir_path = './data'\n",
    "report_dir_path = './reports'\n",
    "model_dir_path = './models'\n",
    "very_large_data_dir_path = './very_large_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up configuration so that GPU is utilised\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True,\n",
    "                        device_count={'CPU': 1, 'GPU': 1})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the Fake or real news dataset\n",
    "art1 = pd.read_csv(\"data/fake_or_real_news.csv\")\n",
    "art1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Watch The Exact Moment Paul Ryan Committed Political Suicide At A Trump Rally (VIDEO)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check\n",
    "art1['title'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us keep out 64 articles as hold out data, which we would later use to make validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 6,271 rows 4 columns\n",
      "Test: 64 rows 4 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2378</td>\n",
       "      <td>Here's why creating single-payer health care i...</td>\n",
       "      <td>The Hillary Clinton campaign is taking some ha...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>9621</td>\n",
       "      <td>Here Are Six ‘Miracle’ Drugs Big Pharma Now Re...</td>\n",
       "      <td>Here Are Six ‘Miracle’ Drugs Big Pharma Now Re...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>5810</td>\n",
       "      <td>Refusal to Acknowledge Uniqueness of Holocaust...</td>\n",
       "      <td>Diversity Macht Frei October 27, 2016 \\nThe Je...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                              title  \\\n",
       "1345        2378  Here's why creating single-payer health care i...   \n",
       "6301        9621  Here Are Six ‘Miracle’ Drugs Big Pharma Now Re...   \n",
       "2715        5810  Refusal to Acknowledge Uniqueness of Holocaust...   \n",
       "\n",
       "                                                   text label  \n",
       "1345  The Hillary Clinton campaign is taking some ha...  REAL  \n",
       "6301  Here Are Six ‘Miracle’ Drugs Big Pharma Now Re...  FAKE  \n",
       "2715  Diversity Macht Frei October 27, 2016 \\nThe Je...  FAKE  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#spliting the data\n",
    "traindf, hold_out = train_test_split(art1, test_size=.01)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {hold_out.shape[0]:,} rows {hold_out.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = traindf['text']\n",
    "Y = traindf.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We limit the vocabulary size and length of the passage to 8000 and 500 words. Similary for the target (title) we will be considering only 2000 words for vocabulary and length of 50 words.\n",
    "\n",
    "Glove word embedding of 100 dimensions is considered due to system and memory constraints. \n",
    "\n",
    "We set the hidden layers to 100, and batch size of 16 for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 8000\n",
    "MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "#Dimension for the Glove Word Embeddings\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "\n",
    "HIDDEN_UNITS = 100  #Hidden Layers\n",
    "batch_size = 16 #batch size for the stochastic gradient descent\n",
    "VERBOSE = 1\n",
    "LOAD_EXISTING_WEIGHTS = True\n",
    "\n",
    "model_name = 'seq2seq_100L_100D_16'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the configuration file for the model which is fitted on the text. Both for target and input, top most frequently occuring words are assigned with an index and then dictionaries are created for the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = fit_text(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to loading the Glove word embedding data from the path specified\n",
    "def load_glove(data_dir_path=None):\n",
    "    if data_dir_path is None:\n",
    "        data_dir_path = 'very_large_data'\n",
    "    download_glove(data_dir_path)\n",
    "    word2em = {}\n",
    "    glove_model_path = data_dir_path + \"/glove.6B.\" + str(GLOVE_EMBEDDING_SIZE) + \"d.txt\"\n",
    "    file = open(glove_model_path, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        word2em[word] = embeds\n",
    "    file.close()\n",
    "    return word2em\n",
    "\n",
    "def glove_zero_emb():\n",
    "    return np.zeros(shape=GLOVE_EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning values to from config file to variables\n",
    "\n",
    "max_input_seq_length = config['max_input_seq_length']\n",
    "num_target_tokens = config['num_target_tokens']\n",
    "max_target_seq_length = config['max_target_seq_length']\n",
    "target_word2idx = config['target_word2idx']\n",
    "target_idx2word = config['target_idx2word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random vector of 50 dimensions is created is added into the config file, these random is utilised to represent the word that is not present in vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for presence of unknown_emb else creating a new random vector and adding to the config file\n",
    "word2em = dict()\n",
    "if 'unknown_emb' in config:\n",
    "    unknown_emb = config['unknown_emb']\n",
    "else:\n",
    "    unknown_emb = np.random.rand(1, GLOVE_EMBEDDING_SIZE)\n",
    "    config['unknown_emb'] = unknown_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data needs to be transformed into embedding layer form. Each word being replaced with its word vector from the glove embeddings. Any word not present in vocabulary is replaced with a common unknown vector. If the total length of input is less than maximum length of input then it is padded with zeros in the front.\n",
    "\n",
    "For the target, title is added with 'START' and 'END' at beggining and end of sentence respectively for letting decoder model the start and terminate the prediction of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation code - text to encoding for the content (input)\n",
    "def transform_input_text(texts):\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = np.zeros(shape=(max_input_seq_length, GLOVE_EMBEDDING_SIZE))\n",
    "        for idx, word in enumerate(line.lower().split(' ')):\n",
    "            if idx >= max_input_seq_length:\n",
    "                break\n",
    "            emb = unknown_emb\n",
    "            if word in word2em:\n",
    "                emb = word2em[word]\n",
    "            x[idx, :] = emb\n",
    "        temp.append(x)\n",
    "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
    "\n",
    "    print(temp.shape)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation code - text to encoding for the title (output)\n",
    "def transform_target_encoding(texts):\n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        for word in line2.split(' '):\n",
    "            x.append(word)\n",
    "            if len(x) >= max_target_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    print(temp.shape)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to generate the batch samples\n",
    "def generate_batch(x_samples, y_samples, batch_size):\n",
    "    num_batches = len(x_samples) // batch_size\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * batch_size\n",
    "            end = (batchIdx + 1) * batch_size\n",
    "            encoder_input_data_batch = pad_sequences(x_samples[start:end], max_input_seq_length)\n",
    "            decoder_target_data_batch = np.zeros(shape=(batch_size, max_target_seq_length, num_target_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(batch_size, max_target_seq_length, num_target_tokens))\n",
    "            for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0  # default [UNK]\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w2idx != 0:\n",
    "                        decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                        if idx > 0:\n",
    "                            decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size:  5016\n",
      "testing size:  1255\n"
     ]
    }
   ],
   "source": [
    "#Splitting the training and validation data\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('training size: ', len(Xtrain))\n",
    "print('testing size: ', len(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(None, GLOVE_EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None, num_target_tokens), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(units=num_target_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None, 2001)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 100), (None, 80400       encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 100),  840800      decoder_inputs[0][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 2001)   202101      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,123,301\n",
      "Trainable params: 1,123,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3098"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the config and model architecure\n",
    "\n",
    "config_file_path = model_dir_path + '/' + model_name + '-config.npy'\n",
    "architecture_file_path = model_dir_path + '/' + model_name + '-architecture.json'\n",
    "\n",
    "np.save(config_file_path, config)\n",
    "open(architecture_file_path, 'w').write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n",
      "(1255,)\n",
      "(5016, 500, 100)\n",
      "(1255, 500, 100)\n"
     ]
    }
   ],
   "source": [
    "#Transforming the data\n",
    "Ytrain = transform_target_encoding(Ytrain)\n",
    "Ytest = transform_target_encoding(Ytest)\n",
    "\n",
    "Xtrain = transform_input_text(Xtrain)\n",
    "Xtest = transform_input_text(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the number of training batches based on batch size\n",
    "train_num_batches = len(Xtrain) // batch_size\n",
    "test_num_batches = len(Xtest) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the batches data\n",
    "train_gen = generate_batch(Xtrain, Ytrain, batch_size)\n",
    "test_gen = generate_batch(Xtest, Ytest, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning weights save path and checkpoint for model to save weights after every epoch\n",
    "weight_file_path = model_dir_path + '/' + model_name + 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(weight_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/313 [==============================] - 182s 580ms/step - loss: 1.0615 - acc: 0.0219 - val_loss: 1.0397 - val_acc: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      "313/313 [==============================] - 176s 564ms/step - loss: 1.0324 - acc: 0.0227 - val_loss: 1.0323 - val_acc: 0.0229\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 1.0197 - acc: 0.0230 - val_loss: 1.0282 - val_acc: 0.0236\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 1.0091 - acc: 0.0237 - val_loss: 1.0237 - val_acc: 0.0244\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.9947 - acc: 0.0246 - val_loss: 1.0210 - val_acc: 0.0252\n",
      "Epoch 6/30\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.9880 - acc: 0.0255 - val_loss: 1.0132 - val_acc: 0.0264\n",
      "Epoch 7/30\n",
      "313/313 [==============================] - 180s 575ms/step - loss: 0.9676 - acc: 0.0271 - val_loss: 1.0036 - val_acc: 0.0270\n",
      "Epoch 8/30\n",
      "313/313 [==============================] - 185s 590ms/step - loss: 0.9516 - acc: 0.0277 - val_loss: 0.9992 - val_acc: 0.0279\n",
      "Epoch 9/30\n",
      "313/313 [==============================] - 180s 574ms/step - loss: 0.9400 - acc: 0.0285 - val_loss: 0.9934 - val_acc: 0.0284\n",
      "Epoch 10/30\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.9310 - acc: 0.0293 - val_loss: 0.9973 - val_acc: 0.0286\n",
      "Epoch 11/30\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.9211 - acc: 0.0300 - val_loss: 0.9892 - val_acc: 0.0293\n",
      "Epoch 12/30\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.9074 - acc: 0.0309 - val_loss: 0.9855 - val_acc: 0.0298\n",
      "Epoch 13/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.8961 - acc: 0.0317 - val_loss: 0.9851 - val_acc: 0.0300\n",
      "Epoch 14/30\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.8865 - acc: 0.0326 - val_loss: 0.9913 - val_acc: 0.0297\n",
      "Epoch 15/30\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.8785 - acc: 0.0332 - val_loss: 0.9847 - val_acc: 0.0306\n",
      "Epoch 16/30\n",
      "313/313 [==============================] - 181s 580ms/step - loss: 0.8674 - acc: 0.0340 - val_loss: 0.9833 - val_acc: 0.0305\n",
      "Epoch 17/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.8577 - acc: 0.0349 - val_loss: 0.9856 - val_acc: 0.0307\n",
      "Epoch 18/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.8496 - acc: 0.0357 - val_loss: 0.9874 - val_acc: 0.0309\n",
      "Epoch 19/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.8419 - acc: 0.0365 - val_loss: 0.9867 - val_acc: 0.0306\n",
      "Epoch 20/30\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.8336 - acc: 0.0374 - val_loss: 0.9873 - val_acc: 0.0303\n",
      "Epoch 21/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.8260 - acc: 0.0382 - val_loss: 0.9900 - val_acc: 0.0302\n",
      "Epoch 22/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.8172 - acc: 0.0389 - val_loss: 0.9922 - val_acc: 0.0298\n",
      "Epoch 23/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.8097 - acc: 0.0398 - val_loss: 0.9966 - val_acc: 0.0291\n",
      "Epoch 24/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.8036 - acc: 0.0404 - val_loss: 0.9963 - val_acc: 0.0299\n",
      "Epoch 25/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.7961 - acc: 0.0411 - val_loss: 1.0005 - val_acc: 0.0282\n",
      "Epoch 26/30\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.7902 - acc: 0.0418 - val_loss: 0.9986 - val_acc: 0.0292\n",
      "Epoch 27/30\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.7824 - acc: 0.0428 - val_loss: 1.0031 - val_acc: 0.0284\n",
      "Epoch 28/30\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.7765 - acc: 0.0434 - val_loss: 1.0055 - val_acc: 0.0293\n",
      "Epoch 29/30\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.7696 - acc: 0.0439 - val_loss: 1.0082 - val_acc: 0.0281\n",
      "Epoch 30/30\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.7640 - acc: 0.0445 - val_loss: 1.0089 - val_acc: 0.0287\n"
     ]
    }
   ],
   "source": [
    "#Traning the Model\n",
    "history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs= 30,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./models/seq2seq_100L_100D_16weights.20-1.05.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.7584 - acc: 0.0454 - val_loss: 1.0081 - val_acc: 0.0299\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 178s 570ms/step - loss: 0.7508 - acc: 0.0464 - val_loss: 1.0096 - val_acc: 0.0285\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.7468 - acc: 0.0470 - val_loss: 1.0117 - val_acc: 0.0295\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 175s 558ms/step - loss: 0.7404 - acc: 0.0474 - val_loss: 1.0126 - val_acc: 0.0295\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.7342 - acc: 0.0486 - val_loss: 1.0196 - val_acc: 0.0288\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.7293 - acc: 0.0492 - val_loss: 1.0186 - val_acc: 0.0295\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 174s 557ms/step - loss: 0.7253 - acc: 0.0497 - val_loss: 1.0202 - val_acc: 0.0287\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.7188 - acc: 0.0508 - val_loss: 1.0301 - val_acc: 0.0287\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.7186 - acc: 0.0510 - val_loss: 1.0266 - val_acc: 0.0285\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.7097 - acc: 0.0521 - val_loss: 1.0288 - val_acc: 0.0284\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.7028 - acc: 0.0533 - val_loss: 1.0325 - val_acc: 0.0293\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6982 - acc: 0.0537 - val_loss: 1.0344 - val_acc: 0.0278\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6945 - acc: 0.0542 - val_loss: 1.0354 - val_acc: 0.0282\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.6919 - acc: 0.0549 - val_loss: 1.0389 - val_acc: 0.0293\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6867 - acc: 0.0554 - val_loss: 1.0416 - val_acc: 0.0292\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6819 - acc: 0.0562 - val_loss: 1.0456 - val_acc: 0.0286\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.6771 - acc: 0.0569 - val_loss: 1.0507 - val_acc: 0.0276\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.6738 - acc: 0.0575 - val_loss: 1.0539 - val_acc: 0.0275\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.6680 - acc: 0.0582 - val_loss: 1.0531 - val_acc: 0.0279\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 176s 564ms/step - loss: 0.6625 - acc: 0.0591 - val_loss: 1.0550 - val_acc: 0.0280\n"
     ]
    }
   ],
   "source": [
    "#Traning the Model\n",
    "history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs= 20,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"./models/seq2seq_100L_100D_16weights.20-1.05.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "313/313 [==============================] - 176s 561ms/step - loss: 0.6595 - acc: 0.0595 - val_loss: 1.0537 - val_acc: 0.0278\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 100) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6545 - acc: 0.0603 - val_loss: 1.0582 - val_acc: 0.0275\n",
      "Epoch 3/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.6496 - acc: 0.0610 - val_loss: 1.0677 - val_acc: 0.0277\n",
      "Epoch 4/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6454 - acc: 0.0615 - val_loss: 1.0690 - val_acc: 0.0269\n",
      "Epoch 5/150\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6424 - acc: 0.0619 - val_loss: 1.0673 - val_acc: 0.0277\n",
      "Epoch 6/150\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6390 - acc: 0.0626 - val_loss: 1.0683 - val_acc: 0.0275\n",
      "Epoch 7/150\n",
      "313/313 [==============================] - 175s 558ms/step - loss: 0.6346 - acc: 0.0633 - val_loss: 1.0782 - val_acc: 0.0277\n",
      "Epoch 8/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6342 - acc: 0.0634 - val_loss: 1.0748 - val_acc: 0.0275\n",
      "Epoch 9/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6302 - acc: 0.0639 - val_loss: 1.0783 - val_acc: 0.0267\n",
      "Epoch 10/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.6262 - acc: 0.0646 - val_loss: 1.0799 - val_acc: 0.0269\n",
      "Epoch 11/150\n",
      "313/313 [==============================] - 175s 558ms/step - loss: 0.6268 - acc: 0.0646 - val_loss: 1.0803 - val_acc: 0.0273\n",
      "Epoch 12/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.6187 - acc: 0.0658 - val_loss: 1.0922 - val_acc: 0.0252\n",
      "Epoch 13/150\n",
      "313/313 [==============================] - 175s 561ms/step - loss: 0.6162 - acc: 0.0665 - val_loss: 1.0849 - val_acc: 0.0270\n",
      "Epoch 14/150\n",
      "313/313 [==============================] - 175s 558ms/step - loss: 0.6116 - acc: 0.0670 - val_loss: 1.0901 - val_acc: 0.0267\n",
      "Epoch 15/150\n",
      "313/313 [==============================] - 175s 559ms/step - loss: 0.6072 - acc: 0.0678 - val_loss: 1.0904 - val_acc: 0.0269\n",
      "Epoch 16/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.6114 - acc: 0.0675 - val_loss: 1.0909 - val_acc: 0.0268\n",
      "Epoch 17/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.6019 - acc: 0.0687 - val_loss: 1.0973 - val_acc: 0.0267\n",
      "Epoch 18/150\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.6035 - acc: 0.0686 - val_loss: 1.1011 - val_acc: 0.0265\n",
      "Epoch 19/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.5986 - acc: 0.0697 - val_loss: 1.0933 - val_acc: 0.0269\n",
      "Epoch 20/150\n",
      "313/313 [==============================] - 176s 561ms/step - loss: 0.5938 - acc: 0.0701 - val_loss: 1.0988 - val_acc: 0.0266\n",
      "Epoch 21/150\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.5906 - acc: 0.0710 - val_loss: 1.1026 - val_acc: 0.0269\n",
      "Epoch 22/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.5919 - acc: 0.0708 - val_loss: 1.1062 - val_acc: 0.0267\n",
      "Epoch 23/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.5870 - acc: 0.0719 - val_loss: 1.1047 - val_acc: 0.0270\n",
      "Epoch 24/150\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.5824 - acc: 0.0724 - val_loss: 1.1072 - val_acc: 0.0265\n",
      "Epoch 25/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.5858 - acc: 0.0720 - val_loss: 1.1118 - val_acc: 0.0263\n",
      "Epoch 26/150\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.5769 - acc: 0.0736 - val_loss: 1.1126 - val_acc: 0.0264\n",
      "Epoch 27/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.5745 - acc: 0.0739 - val_loss: 1.1168 - val_acc: 0.0264\n",
      "Epoch 28/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.5721 - acc: 0.0743 - val_loss: 1.1191 - val_acc: 0.0267\n",
      "Epoch 29/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5715 - acc: 0.0744 - val_loss: 1.1194 - val_acc: 0.0262\n",
      "Epoch 30/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.5693 - acc: 0.0750 - val_loss: 1.1225 - val_acc: 0.0261\n",
      "Epoch 31/150\n",
      "313/313 [==============================] - 176s 561ms/step - loss: 0.5660 - acc: 0.0757 - val_loss: 1.1214 - val_acc: 0.0264\n",
      "Epoch 32/150\n",
      "313/313 [==============================] - 176s 561ms/step - loss: 0.5647 - acc: 0.0760 - val_loss: 1.1210 - val_acc: 0.0262\n",
      "Epoch 33/150\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.5659 - acc: 0.0756 - val_loss: 1.1271 - val_acc: 0.0263\n",
      "Epoch 34/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.5582 - acc: 0.0769 - val_loss: 1.1264 - val_acc: 0.0261\n",
      "Epoch 35/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5602 - acc: 0.0767 - val_loss: 1.1329 - val_acc: 0.0264\n",
      "Epoch 36/150\n",
      "313/313 [==============================] - 175s 560ms/step - loss: 0.5540 - acc: 0.0784 - val_loss: 1.1282 - val_acc: 0.0259\n",
      "Epoch 37/150\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.5519 - acc: 0.0783 - val_loss: 1.1337 - val_acc: 0.0258\n",
      "Epoch 38/150\n",
      "313/313 [==============================] - 177s 565ms/step - loss: 0.5546 - acc: 0.0779 - val_loss: 1.1426 - val_acc: 0.0252\n",
      "Epoch 39/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5474 - acc: 0.0794 - val_loss: 1.1382 - val_acc: 0.0260\n",
      "Epoch 40/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5454 - acc: 0.0797 - val_loss: 1.1403 - val_acc: 0.0263\n",
      "Epoch 41/150\n",
      "313/313 [==============================] - 176s 562ms/step - loss: 0.5436 - acc: 0.0798 - val_loss: 1.1374 - val_acc: 0.0258\n",
      "Epoch 42/150\n",
      "313/313 [==============================] - 176s 561ms/step - loss: 0.5439 - acc: 0.0798 - val_loss: 1.1497 - val_acc: 0.0257\n",
      "Epoch 43/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5406 - acc: 0.0805 - val_loss: 1.1437 - val_acc: 0.0257\n",
      "Epoch 44/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5366 - acc: 0.0813 - val_loss: 1.1450 - val_acc: 0.0263\n",
      "Epoch 45/150\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.5340 - acc: 0.0819 - val_loss: 1.1496 - val_acc: 0.0258\n",
      "Epoch 46/150\n",
      "313/313 [==============================] - 176s 563ms/step - loss: 0.5317 - acc: 0.0824 - val_loss: 1.1476 - val_acc: 0.0259\n",
      "Epoch 47/150\n",
      "313/313 [==============================] - 177s 564ms/step - loss: 0.5311 - acc: 0.0823 - val_loss: 1.1533 - val_acc: 0.0255\n",
      "Epoch 48/150\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.5309 - acc: 0.0824 - val_loss: 1.1564 - val_acc: 0.0254\n",
      "Epoch 49/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.5280 - acc: 0.0827 - val_loss: 1.1556 - val_acc: 0.0257\n",
      "Epoch 50/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.5285 - acc: 0.0825 - val_loss: 1.1612 - val_acc: 0.0260\n",
      "Epoch 51/150\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.5245 - acc: 0.0836 - val_loss: 1.1593 - val_acc: 0.0260\n",
      "Epoch 52/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.5206 - acc: 0.0845 - val_loss: 1.1606 - val_acc: 0.0253\n",
      "Epoch 53/150\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.5191 - acc: 0.0845 - val_loss: 1.1665 - val_acc: 0.0259\n",
      "Epoch 54/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.5231 - acc: 0.0839 - val_loss: 1.1637 - val_acc: 0.0258\n",
      "Epoch 55/150\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.5192 - acc: 0.0850 - val_loss: 1.1747 - val_acc: 0.0258\n",
      "Epoch 56/150\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.5167 - acc: 0.0851 - val_loss: 1.1693 - val_acc: 0.0258\n",
      "Epoch 57/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.5138 - acc: 0.0859 - val_loss: 1.1729 - val_acc: 0.0257\n",
      "Epoch 58/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.5120 - acc: 0.0863 - val_loss: 1.1726 - val_acc: 0.0257\n",
      "Epoch 59/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.5090 - acc: 0.0869 - val_loss: 1.1778 - val_acc: 0.0257\n",
      "Epoch 60/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.5058 - acc: 0.0873 - val_loss: 1.1768 - val_acc: 0.0258\n",
      "Epoch 61/150\n",
      "313/313 [==============================] - 179s 570ms/step - loss: 0.5062 - acc: 0.0873 - val_loss: 1.1787 - val_acc: 0.0256\n",
      "Epoch 62/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.5031 - acc: 0.0880 - val_loss: 1.1756 - val_acc: 0.0252\n",
      "Epoch 63/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.5025 - acc: 0.0880 - val_loss: 1.1802 - val_acc: 0.0256\n",
      "Epoch 64/150\n",
      "313/313 [==============================] - 181s 577ms/step - loss: 0.5031 - acc: 0.0877 - val_loss: 1.1888 - val_acc: 0.0252\n",
      "Epoch 65/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.5017 - acc: 0.0881 - val_loss: 1.1892 - val_acc: 0.0252\n",
      "Epoch 66/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4964 - acc: 0.0894 - val_loss: 1.1860 - val_acc: 0.0256\n",
      "Epoch 67/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4959 - acc: 0.0892 - val_loss: 1.1887 - val_acc: 0.0255\n",
      "Epoch 68/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.4946 - acc: 0.0894 - val_loss: 1.1907 - val_acc: 0.0256\n",
      "Epoch 69/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4915 - acc: 0.0904 - val_loss: 1.1927 - val_acc: 0.0257\n",
      "Epoch 70/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4896 - acc: 0.0903 - val_loss: 1.1998 - val_acc: 0.0248\n",
      "Epoch 71/150\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.4874 - acc: 0.0911 - val_loss: 1.1994 - val_acc: 0.0252\n",
      "Epoch 72/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4859 - acc: 0.0914 - val_loss: 1.2023 - val_acc: 0.0254\n",
      "Epoch 73/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4854 - acc: 0.0917 - val_loss: 1.2038 - val_acc: 0.0252\n",
      "Epoch 74/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4841 - acc: 0.0915 - val_loss: 1.2056 - val_acc: 0.0248\n",
      "Epoch 75/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4872 - acc: 0.0912 - val_loss: 1.2035 - val_acc: 0.0253\n",
      "Epoch 76/150\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.4950 - acc: 0.0890 - val_loss: 1.2006 - val_acc: 0.0252\n",
      "Epoch 77/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4816 - acc: 0.0921 - val_loss: 1.2061 - val_acc: 0.0252\n",
      "Epoch 78/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4774 - acc: 0.0932 - val_loss: 1.2094 - val_acc: 0.0251\n",
      "Epoch 79/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4770 - acc: 0.0934 - val_loss: 1.2109 - val_acc: 0.0250\n",
      "Epoch 80/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.4731 - acc: 0.0942 - val_loss: 1.2133 - val_acc: 0.0252\n",
      "Epoch 81/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.4731 - acc: 0.0939 - val_loss: 1.2180 - val_acc: 0.0251\n",
      "Epoch 82/150\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.4704 - acc: 0.0942 - val_loss: 1.2213 - val_acc: 0.0253\n",
      "Epoch 83/150\n",
      "313/313 [==============================] - 177s 567ms/step - loss: 0.4717 - acc: 0.0943 - val_loss: 1.2227 - val_acc: 0.0250\n",
      "Epoch 84/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.4692 - acc: 0.0949 - val_loss: 1.2222 - val_acc: 0.0246\n",
      "Epoch 85/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4667 - acc: 0.0955 - val_loss: 1.2278 - val_acc: 0.0250\n",
      "Epoch 86/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4664 - acc: 0.0955 - val_loss: 1.2276 - val_acc: 0.0252\n",
      "Epoch 87/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.4688 - acc: 0.0948 - val_loss: 1.2262 - val_acc: 0.0246\n",
      "Epoch 88/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4645 - acc: 0.0953 - val_loss: 1.2274 - val_acc: 0.0250\n",
      "Epoch 89/150\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.4627 - acc: 0.0961 - val_loss: 1.2300 - val_acc: 0.0249\n",
      "Epoch 90/150\n",
      "313/313 [==============================] - 177s 566ms/step - loss: 0.4610 - acc: 0.0964 - val_loss: 1.2340 - val_acc: 0.0244\n",
      "Epoch 91/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4611 - acc: 0.0961 - val_loss: 1.2342 - val_acc: 0.0248\n",
      "Epoch 92/150\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.4600 - acc: 0.0966 - val_loss: 1.2332 - val_acc: 0.0244\n",
      "Epoch 93/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4605 - acc: 0.0962 - val_loss: 1.2329 - val_acc: 0.0250\n",
      "Epoch 94/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4570 - acc: 0.0971 - val_loss: 1.2434 - val_acc: 0.0248\n",
      "Epoch 95/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4563 - acc: 0.0973 - val_loss: 1.2360 - val_acc: 0.0251\n",
      "Epoch 96/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4568 - acc: 0.0969 - val_loss: 1.2363 - val_acc: 0.0252\n",
      "Epoch 97/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4546 - acc: 0.0975 - val_loss: 1.2352 - val_acc: 0.0249\n",
      "Epoch 98/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4550 - acc: 0.0975 - val_loss: 1.2367 - val_acc: 0.0251\n",
      "Epoch 99/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4546 - acc: 0.0974 - val_loss: 1.2422 - val_acc: 0.0246\n",
      "Epoch 100/150\n",
      "313/313 [==============================] - 180s 575ms/step - loss: 0.4528 - acc: 0.0979 - val_loss: 1.2380 - val_acc: 0.0245\n",
      "Epoch 101/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.4531 - acc: 0.0978 - val_loss: 1.2489 - val_acc: 0.0246\n",
      "Epoch 102/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4539 - acc: 0.0974 - val_loss: 1.2358 - val_acc: 0.0250\n",
      "Epoch 103/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4499 - acc: 0.0984 - val_loss: 1.2399 - val_acc: 0.0249\n",
      "Epoch 104/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4491 - acc: 0.0984 - val_loss: 1.2398 - val_acc: 0.0244\n",
      "Epoch 105/150\n",
      "313/313 [==============================] - 178s 567ms/step - loss: 0.4478 - acc: 0.0988 - val_loss: 1.2436 - val_acc: 0.0246\n",
      "Epoch 106/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4481 - acc: 0.0985 - val_loss: 1.2438 - val_acc: 0.0245\n",
      "Epoch 107/150\n",
      "313/313 [==============================] - 178s 570ms/step - loss: 0.4454 - acc: 0.0993 - val_loss: 1.2460 - val_acc: 0.0248\n",
      "Epoch 108/150\n",
      "313/313 [==============================] - 178s 569ms/step - loss: 0.4455 - acc: 0.0992 - val_loss: 1.2417 - val_acc: 0.0247\n",
      "Epoch 109/150\n",
      "313/313 [==============================] - 179s 571ms/step - loss: 0.4449 - acc: 0.0995 - val_loss: 1.2454 - val_acc: 0.0247\n",
      "Epoch 110/150\n",
      "313/313 [==============================] - 179s 573ms/step - loss: 0.4456 - acc: 0.0991 - val_loss: 1.2444 - val_acc: 0.0246\n",
      "Epoch 111/150\n",
      "313/313 [==============================] - 178s 568ms/step - loss: 0.4422 - acc: 0.1001 - val_loss: 1.2487 - val_acc: 0.0244\n",
      "Epoch 112/150\n",
      "313/313 [==============================] - 179s 572ms/step - loss: 0.4435 - acc: 0.0999 - val_loss: 1.2483 - val_acc: 0.0245\n",
      "Epoch 113/150\n",
      " 44/313 [===>..........................] - ETA: 2:49:32 - loss: 0.4323 - acc: 0.1006"
     ]
    }
   ],
   "source": [
    "#Traning the Model\n",
    "history = model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                           epochs= 150,\n",
    "                                           verbose=VERBOSE, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                           callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "        input_seq = np.zeros(shape=(1, max_input_seq_length, GLOVE_EMBEDDING_SIZE))\n",
    "        for idx, word in enumerate(input_text.lower().split(' ')):\n",
    "            if idx >= max_input_seq_length:\n",
    "                break\n",
    "            emb = unknown_emb  # default [UNK]\n",
    "            if word in word2em:\n",
    "                emb = word2em[word]\n",
    "            input_seq[0, idx, :] = emb\n",
    "        states_value = encoder_model.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1, num_target_tokens))\n",
    "        target_seq[0, 0, target_word2idx['START']] = 1\n",
    "        target_text = ''\n",
    "        target_text_len = 0\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = target_idx2word[sample_token_idx]\n",
    "            target_text_len += 1\n",
    "\n",
    "            if sample_word != 'START' and sample_word != 'END':\n",
    "                target_text += ' ' + sample_word\n",
    "\n",
    "            if sample_word == 'END' or target_text_len >= max_target_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, num_target_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Headline:  california today: today: the u.s. in the the world\n",
      "Original Headline:  Bill Paxton, Star of ‘Big Love’ and Movie Blockbusters, Dies at 61\n",
      "Generated Headline:  north and china is killed by the watch: china\n",
      "Original Headline:  Oakland Fire Victims Included Performers, Educators and Lawyers\n",
      "Generated Headline:  china says u.s. american american have\n",
      "Original Headline:  ‘Hamilton’ Inc.: The Path to a Billion-Dollar Broadway Show\n",
      "Generated Headline:  a new york times with a new york times\n",
      "Original Headline:  Mute and Alone, He Was Never Short of Kind Words or Friends\n",
      "Generated Headline:  breitbart news daily: daily: trump\n",
      "Original Headline:  The Budget Funds 99 Things and a Wall Ain’t One\n",
      "Generated Headline:  to donald trump and a new york ban\n",
      "Original Headline:  Cities Vow to Fight Trump on Immigration, Even if They Lose Millions\n",
      "Generated Headline:  watch: attorney general is not to be days\n",
      "Original Headline:  WATCH: Anti-MILO Protesters Tear Down Barricades At UC Davis\n",
      "Generated Headline:  china and china is charged in china\n",
      "Original Headline:  Egyptian Court Clears Way for Hosni Mubarak’s Release\n",
      "Generated Headline:  a new york times with a new york\n",
      "Original Headline:  ‘Here Lies’: A Clue in Hebrew Points to Rome’s Medieval Jewish Cemetery\n",
      "Generated Headline:  a new york times with a new york city\n",
      "Original Headline:  Green Water Lingers in Olympic Pools as the Excuses Pile Up\n",
      "Generated Headline:  china to u.s. to make it to be back on\n",
      "Original Headline:  Google Faces New Round of Antitrust Charges in Europe\n",
      "Generated Headline:  watch: attorney general is not to be days\n",
      "Original Headline:  Pelosi: Case Being Made ’In a Very Scientific, Methodical Way’ to Impeach Trump\n",
      "Generated Headline:  a new york times with a new york times\n",
      "Original Headline:  DELINGPOLE: Facebook Banned Me For Defending Milo\n",
      "Generated Headline:  in a new york times are isis with isis\n",
      "Original Headline:  Opinion Transforms Texas’ Abortion Landscape\n",
      "Generated Headline:  u.s. says u.s. to north korea on u.s. to fight to email email email\n",
      "Original Headline:  House Challenge to Health Law Could Raise Premiums, Administration Says\n",
      "Generated Headline:  trump to wednesday at wall\n",
      "Original Headline:  California Today: The Tale of the Laguna Beach Jumper\n",
      "Generated Headline:  new york times at least pain\n",
      "Original Headline:  Review: ‘Hairspray Live!’ Had Power Voices but Still Lacked Power\n",
      "Generated Headline:  u.s. u.s. will be killed in isis\n",
      "Original Headline:  Belgium Says It Prevented a Terror Attack on Soccer Fans\n",
      "Generated Headline:  china and a new new york times with the times\n",
      "Original Headline:  Colin Kaepernick’s Anthem Protest Underlines Union of Sports and Patriotism\n",
      "Generated Headline:  to donald trump\n",
      "Original Headline:  Zika, Olympics, U.S. Presidential Race: Your Weekend Briefing\n"
     ]
    }
   ],
   "source": [
    "print('start predicting ...')\n",
    "for i in np.random.permutation(np.arange(len(X)))[0:20]:\n",
    "    x = X[i]\n",
    "    actual_headline = Y[i]\n",
    "    headline = summarize(x)\n",
    "\n",
    "    print('Generated Headline: ', headline)\n",
    "    print('Original Headline: ', actual_headline)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
