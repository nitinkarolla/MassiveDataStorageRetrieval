{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive Data Storage and Retrieval\n",
    "\n",
    "\n",
    "## Final Report Submission\n",
    "\n",
    "#### Submited by Nitin Reddy Karolla (nrk60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### “Heading/Sub-Heading identification for provided paragraph”\n",
    "\n",
    "\n",
    "The aim of this project is to develop a methodology that can be used to assign a title to a paragraph provided. \n",
    "Here we will try to build a corpus containing paragraphs of different sizes. Data will be picked up from various sources like Wikipedia, News Articles, Journals, and Forums etc. Following two sub-objectives will be kept in mind while trying to develop a methodology. \n",
    "\n",
    "        \n",
    "    •\tFinding a single word from the paragraph that can be a title\n",
    "    •\tFinding the new word that has not occurred in the paragraph to be a title\n",
    "    \n",
    "Different approaches will be tried in identifying the title for two sub-objectives. We intend to read through the paragraph and identify a word or apply chunker that can best describe the whole paragraph. Also, we intend to give a title that could not be a word present in the paragraph. \n",
    "\n",
    "Paragraphs are going to parsed independently and titles will be predicted without any knowledge of the pre or past occurring paragraphs. The research will involve identifying any relevant research and developing methodology using Natural Language Tools such as word vectors, tf-idf, word-net, pos taggers and but not just restricted to this. The scope of applicability of Machine learning will also be considered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there is no exactly same work conducted before as I am trying to achieve here, so the data is not readily available. I have put together some data collected form different sources. Close to 40 paragraphs are randomly collected from wikipedia articles each of close 5-10 sentences. Also sample of 250 news article is collected from the 'All the News' kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the Paragraph Data\n",
    "para_data1 = pd.read_csv('E:/NLP/Code/data_40.csv')\n",
    "\n",
    "news = pd.read_csv(\"E:\\\\NLP\\\\Data\\\\articles1.csv\")\n",
    "sample = news['content'].sample(250, random_state = 22).reset_index(drop= True)\n",
    "\n",
    "#Combine data\n",
    "data = pd.DataFrame(para_data1[\"Paragraph\"].append(sample).reset_index(drop= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns =['para']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (CNN) If you see a country music fan this week, you may want to give them a hug. If Twitter is any indication, folks are grieving over the announcement that Blake Shelton and Miranda Lambert, the first couple of country music, are splitting after four years of marriage. Some of it may be    or hyperbole, but news of the breakup has been accompanied by the type of sadness worthy of a country hit. Country stars Blake Shelton and Miranda Lambert split, One person even tweeted, ”Now that Blake Shelton and Miranda Lambert are getting divorced I can say with 100% confidence that love does not exist.” Of course, along with the grief comes tons of speculation about what may have happened in the relationship between ”The Voice” coach and his superstar wife.  Much is being made of Lambert’s tearful performance of ”The House That Built Me” at Cheyenne Frontier Days in Wyoming on Saturday, days before the breakup was announced. Shelton told People magazine that he played the song    which was originally written for him    for Lambert one night during a drive when the pair were engaged. She burst into tears, he said, and he decided to give the song to her to record.  ”I took one look at her and just said, ’If you have this strong of a reaction to a song, you need to record it!’ ” Shelton told the publication in 2011. \n"
     ]
    }
   ],
   "source": [
    "#Let us look at one random paragraph from Wikipedia paragraphs\n",
    "print(sample[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers typically implement plug-in functionality using shared libraries, which get dynamically loaded at run time, installed in a place prescribed by the host application. HyperCard supported a similar facility, but more commonly included the plug-in code in the HyperCard documents (called stacks) themselves. Thus the HyperCard stack became a self-contained application in its own right, distributable as a single entity that end-users could run without the need for additional installation-steps. Programs may also implement plugins by loading a directory of simple script files written in a scripting language like Python or Lua.\n"
     ]
    }
   ],
   "source": [
    "#Let us look at one random paragraph from Wikipedia paragraphs\n",
    "print(para_data1['Paragraph'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea here is to identify title for the paragraphs, so we will try to fetch the central idea of the paragraph which could be suggested as the title for the paragraph. There can be lot of ways in which the central theme and idea of the paragraph can be obtained. Lets us look at one very naive approach using the words embeddings.\n",
    "\n",
    "Word embedding are being used in the various forms in field of NLP. Considering word embeddings to calculate the paragraph vector can be a good start to the problem. Using the paragraph vector to check the closest word in the word embedding can be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 1 - Paragraph Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post cleaning the paragraphs i.e. after removing the special characters, we tokenise the paragraph into words. Now, we remove the stop words as they generally dont really have strong meaning in the sentence or paragraph. Once we have the words, we can fetch the words vector for each of these words and calculate the average of the all word vector to obtain the paragraph vector. Post which we look for the top 10 word vectors that close to the paragraph vector and analyse it.\n",
    "\n",
    "I have here considered the Glove Word Vector of 300d to calcuate the paragraph vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Importing the required packages\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import collections\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcation to load the Word Embedding File\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding=\"utf8\")\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. 400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "#Reading the 300d Glove 6B word vector file\n",
    "wordvector = loadGloveModel(\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Clearning and formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = data['para'].apply(lambda x : re.sub('[^A-Za-z]+', ' ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here plan to keep only the strings i.e. remove all the characters that are not in alphabets including numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Bach family already counted several composers when Johann Sebastian was born as the last child of a city musician in Eisenach. After becoming an orphan at age 10, he lived for five years with his eldest brother, after which he continued his musical development in Lüneburg. From 1703 he was back in Thuringia, working as a musician for Protestant churches in Arnstadt and Mühlhausen and, for longer stretches of time, at courts in Weimar—where he expanded his repertoire for the organ—and Köthen—where he was mostly engaged with chamber music. From 1723 he was employed as Thomaskantor (cantor at St. Thomas) in Leipzig. He composed music for the principal Lutheran churches of the city, and for its university's student ensemble Collegium Musicum. From 1726 he published some of his keyboard and organ music. In Leipzig, as had happened in some of his earlier positions, he had a difficult relation with his employer, a situation that was little remedied when he was granted the title of court composer by the Elector of Saxony and King of Poland in 1736. In the last decades of his life he reworked and extended many of his earlier compositions. He died of complications after eye surgery in 1750.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"para\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Bach family already counted several composers when Johann Sebastian was born as the last child of a city musician in Eisenach After becoming an orphan at age he lived for five years with his eldest brother after which he continued his musical development in L neburg From he was back in Thuringia working as a musician for Protestant churches in Arnstadt and M hlhausen and for longer stretches of time at courts in Weimar where he expanded his repertoire for the organ and K then where he was mostly engaged with chamber music From he was employed as Thomaskantor cantor at St Thomas in Leipzig He composed music for the principal Lutheran churches of the city and for its university s student ensemble Collegium Musicum From he published some of his keyboard and organ music In Leipzig as had happened in some of his earlier positions he had a difficult relation with his employer a situation that was little remedied when he was granted the title of court composer by the Elector of Saxony and King of Poland in In the last decades of his life he reworked and extended many of his earlier compositions He died of complications after eye surgery in '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"cleaned\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a paragraph vector function that would take in a paragraph p and return the average of embedding for the words present in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Paragraph Vector \n",
    "def para_vec(p):\n",
    "    para_list = p.lower().split(\" \")\n",
    "    para_list = [word for word in para_list if word not in stopwords.words('english')]\n",
    "    count = 0\n",
    "    feature_vec = np.zeros((300, ), dtype='float32')\n",
    "    for i in para_list:\n",
    "        if i in wordvector:\n",
    "            count += 1\n",
    "            feature_vec = np.add(feature_vec, wordvector[i])\n",
    "    if (count > 0):\n",
    "        feature_vec = np.divide(feature_vec, count)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the para_vec function on complete cleaned data\n",
    "data['vector'] = data['cleaned'].apply(para_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a word2vec model of the wordembedding so that methods of the word2vec can be easily applied\n",
    "glove_file = datapath('E:\\\\NLP\\\\Code\\\\glove.6B.300d.txt')\n",
    "tmp_file = get_tmpfile(\"test_word2vec.txt\")\n",
    "glove2word2vec(glove_file, tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nitin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "#Fetching the top ten word vectors that close to the paragraph vector\n",
    "data['suggested_topics'] = data['vector'].apply(model.similar_by_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"idea1_output.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "Some observations from Experiment 1 – Average Paragraph Vector Similar Words \n",
    "\n",
    "\n",
    "•\tParagraph 0 – Is a paragraph from article on Wikipedia and we can see that the Titles suggested are somewhat like – ‘Web’, ‘Online’, ‘Internet’. These topics are basically found to be the very general central theme of the paragraph. We can see from the paragraph, that most of the words occurring in the paragraph are related to the ‘Web’, ‘Online’ and ‘Internet’.\n",
    "I have referred such observation with Pattern column marked as Y. Around 16 of 40 wikipedia articles had tittle suggestions slightly close to the content of the paragraph. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The online encyclopedia project Wikipedia is the most popular wiki-based website, and is one of the most widely viewed sites in the world, having been ranked in the top ten since 2007.[3] Wikipedia is not a single wiki but rather a collection of hundreds of wikis, with each one pertaining to a specific language. In addition to Wikipedia, there are tens of thousands of other wikis in use, both public and private, including wikis functioning as knowledge management resources, notetaking tools, community websites, and intranets. The English-language Wikipedia has the largest collection of articles; as of September 2016, it had over five million articles. Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described it as \"the simplest online database that could possibly work\".[4] \"Wiki\" (pronounced [?wiki][note 1]) is a Hawaiian word meaning \"quick\".[5][6][7]\n",
      "Suggested topics for the paragraph are -  [('web', 0.6910759210586548), ('online', 0.6712905764579773), ('example', 0.6659792065620422), ('only', 0.6428586840629578), ('internet', 0.6358155012130737), ('addition', 0.6325753331184387), ('many', 0.6271064281463623), ('use', 0.6264867782592773), ('well', 0.625910222530365), ('instance', 0.6258213520050049)]\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[0,'para'])\n",
    "\n",
    "print(\"Suggested topics for the paragraph are - \",data.loc[0,'suggested_topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tParagraph 7 – central theme was captured well by the tittle suggested talking about ‘education’, ’graduate’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cunningham was born in Michigan City, Indiana and grew up in Highland, Indiana, staying there through high school.[4] He received his Bachelor's degree in interdisciplinary engineering (electrical engineering and computer science) and his master's degree in computer science from Purdue University, graduating in 1978.[5] He is a founder of Cunningham & Cunningham, Inc. He has also served as Director of R&D at Wyatt Software and as Principal Engineer in the Tektronix Computer Research Laboratory. He is founder of The Hillside Group and has served as program chair of the Pattern Languages of Programming conference which it sponsors. Cunningham was part of the Smalltalk community. From December 2003 until October 2005, he worked for Microsoft Corporation in the \"Patterns & Practices\" group. From October 2005 to May 2007, he held the position of Director of Committer Community Development at the Eclipse Foundation.\n",
      "Suggested topics for the paragraph are -  [('university', 0.6784282922744751), ('science', 0.6489746570587158), (',', 0.6472765207290649), ('research', 0.6392382979393005), ('graduate', 0.6260223388671875), ('engineering', 0.6225532293319702), ('technology', 0.6221566796302795), ('education', 0.6161079406738281), ('.', 0.6139761209487915), ('addition', 0.6116740703582764)]\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[7,'para'])\n",
    "\n",
    "print(\"Suggested topics for the paragraph are - \",data.loc[7,'suggested_topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tParagraph 14 – ‘government’ and ‘states’ is a repeating word in the paragraph, so it seems obvious to be present as suggested tittle as the common repeating words skews the paragraph vector. This pattern was common across many paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The states of the Weimar Republic were effectively abolished after the establishment of Nazi Germany in 1933 by a series of Reichsstatthalter decrees between 1933 and 1935, and autonomy was replaced by direct rule of the National Socialist German Workers' Party in the Gleichschaltung process. The states continued to formally exist as de jure rudimentary bodies, but from 1934 were superseded by de facto Nazi provinces called Gaue. Many of the states were formally dissolved at the end of World War II by the Allies, and ultimately re-organised into the modern states of Germany.\n",
      "Suggested topics for the paragraph are -  [('which', 0.6483539342880249), ('states', 0.6448923349380493), ('government', 0.6426990628242493), ('however', 0.6423750519752502), ('part', 0.6297308206558228), ('war', 0.6249045133590698), ('although', 0.621939480304718), ('.', 0.6192313432693481), ('rule', 0.614014744758606), ('governments', 0.6111968755722046)]\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[14,'para'])\n",
    "\n",
    "print(\"Suggested topics for the paragraph are - \",data.loc[14,'suggested_topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tParagraph 28 – Is a short paragraph consisting about 3-4 lines. And titles suggested are interesting and made sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When terrorism began to be treated as a more active threat after the September 11 attacks, BART increased its emphasis on infrastructure protection. The police department hosts drills and participates in counter-terrorism working groups. The agency has an officer assigned full-time to the FBI's Joint Terrorism Task Force. Furthermore, a command officer is designated as a mutual-aid, counter-terrorism, and homeland-security liaison. BART's police dogs are certified in explosives detection.\n",
      "Suggested topics for the paragraph are -  [('security', 0.7271169424057007), ('terrorism', 0.6751623153686523), ('enforcement', 0.6623741388320923), ('officials', 0.6602051258087158), ('military', 0.6486713886260986), ('forces', 0.6454729437828064), ('force', 0.6323009729385376), ('officers', 0.6316796541213989), ('terrorist', 0.6303951144218445), ('police', 0.6276746988296509)]\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[28,'para'])\n",
    "\n",
    "print(\"Suggested topics for the paragraph are - \",data.loc[28,'suggested_topics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tMost frequent title suggested were ‘example’, ‘many’, ‘well’, ‘because’, ‘same’, ‘used’, 'but', 'that' for most of the paragraphs including the new articles (except very few news articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'online'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['suggested_topics'][0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('but', 248),\n",
       " ('that', 205),\n",
       " ('even', 197),\n",
       " ('because', 185),\n",
       " ('not', 152),\n",
       " ('what', 133),\n",
       " ('so', 122),\n",
       " ('.', 101),\n",
       " ('they', 98),\n",
       " ('would', 95),\n",
       " ('only', 64),\n",
       " ('saying', 52),\n",
       " ('should', 51),\n",
       " ('come', 43),\n",
       " ('well', 42),\n",
       " (\"n't\", 40),\n",
       " ('same', 37),\n",
       " ('one', 36),\n",
       " ('when', 35),\n",
       " ('last', 33)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the most frequent title suggested\n",
    "wordcount={}\n",
    "for i in data['suggested_topics']:\n",
    "    for j in i:\n",
    "        if j[0] not in wordcount:\n",
    "            wordcount[j[0]] = 1\n",
    "        else:\n",
    "            wordcount[j[0]] += 1\n",
    "\n",
    "topics = collections.Counter(wordcount)\n",
    "topics.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 2 - Extracting subject matter (first step of chunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify title we need to understand what the subject is talking about. Extracting Subject out of a paragraph can be really helpful as this is the main subject of the paragraph. Currently we are expecting the subject to be the named entity among the nouns present in the paragraph. \n",
    "\n",
    "We will extract the subject main word or phrase by first calculating the word frequency distribution. Then, the most frequent nouns are collected. We use NLTK's built-in methods to achieve that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Noun Part of Speech Tags used by NLTK\n",
    "NOUNS = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "VERBS = ['VB', 'VBG', 'VBD', 'VBN', 'VBP', 'VBZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us define some set of functions that can be used to extract the subject out of the paragraph provided. First of all, we need a function that will do the cleaning of the paragraph passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the paragraph\n",
    "def clean_document(paragraph):\n",
    "    paragraph = re.sub('[^A-Za-z .-]+', ' ', paragraph)\n",
    "    paragraph = ' '.join(paragraph.split())\n",
    "    paragraph = ' '.join([i for i in paragraph.split() if i not in stop])\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us define a function, which when called on paragraph will tokenize the sentences present in the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the sentences\n",
    "def tokenize_sentences(paragraph):\n",
    "    sentences = nltk.sent_tokenize(paragraph)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting entities would be a main step, if we want to identify the subject. We believe that the majority of the subject to be a named entity. To extract Named Entity we use the 'ne_chunk' along with 'pos_tag' from the package NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Named Entities using NLTK Chinking\n",
    "def get_entities(paragraph):\n",
    "    entities = []\n",
    "    sentences = tokenize_sentences(paragraph)\n",
    "\n",
    "    # Part of Speech Tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in nltk.ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == nltk.tree.Tree:\n",
    "                entities.append(' '.join([c[0] for c in chunk]).lower())\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we would like to know frequency of the word that are present in paragraph, the importance of this step will be eventually clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the Word count frequency\n",
    "def word_freq_dist(paragraph):\n",
    "    words = nltk.tokenize.word_tokenize(paragraph)\n",
    "    words = [word.lower() for word in words if word not in stop]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    return fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the subject of the paragraph should be occuring majority of times. Any named entity present multiple times in paragraph and which is a noun is expected to be Subject of the paragraph than a named entity present one or two times. Based on this logic we try to find the subject which is most commonly repeating and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the main subject out of the paragraph\n",
    "def extract_subject(document):\n",
    "    # Get most frequent Nouns\n",
    "    fdist = word_freq_dist(document)\n",
    "    most_freq_nouns = [w for w, c in fdist.most_common(10)\n",
    "                       if nltk.pos_tag([w])[0][1] in NOUNS]\n",
    "\n",
    "    # Get Top 10 entities\n",
    "    entities = get_entities(document)\n",
    "    top_10_entities = [w for w, c in nltk.FreqDist(entities).most_common(10)]\n",
    "\n",
    "    # Get the subject noun by looking at the intersection of top 10 entities\n",
    "    # and most frequent nouns. It takes the first element in the list\n",
    "    subject_nouns = [entity for entity in top_10_entities\n",
    "                    if entity.split()[0] in most_freq_nouns]\n",
    "    #print (subject_nouns)\n",
    "    return subject_nouns[0]\n",
    "\n",
    "def for_all_subjects(x):\n",
    "    try:\n",
    "        return extract_subject(x)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['subject_nouns'] = data['para'].apply(for_all_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the output and analyse it. As we can see below, - 'Gerald' is the main subject in the paragraph provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Upon the death of his uncle, the Bishop of St David\\'s, in 1176, the chapter nominated Gerald as his successor. St David\\'s had the long-term aim of becoming independent of Canterbury, and the chapter may have thought that Gerald was the man to take up its cause. Henry II of England, fresh from his struggle with Thomas Becket, promptly rejected Gerald, possibly because his Welsh blood and ties to the ruling family of Deheubarth made him seem like a troublesome prospect, in favour of one of his Norman retainers Peter de Leia. According to Gerald, the king said at the time: \"It is neither necessary or expedient for king or archbishop that a man of great honesty or vigour should become Bishop of St. David\\'s, for fear that the Crown and Canterbury should suffer thereby. Such an appointment would only give strength to the Welsh and increase their pride\".[3] The chapter acquiesced in the decision; and Gerald, disappointed with the result, withdrew to the University of Paris. From c.?1179-8, he studied and taught canon law and theology. He returned to England and spent an additional five years studying theology. In 1180, he received a minor appointment from the Bishop of St. David\\'s, which he soon resigned.[1]'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['para'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gerald'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subject_nouns'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (CNN) Another ePrix, another victory for Sebastien Buemi.  The Renault eDams driver made it three wins out of three for the   season at the Buenos Aires ePrix with another impressive drive at the Puerto Madero Street Circuit on Saturday.  The reigning world champion led for the majority of the   race after starting third on the grid behind pole sitter Lucas di Grassi    a first for the ABT Schaeffler Audi Sport driver    and Techeetah’s   Vergne.  Vergne seized the lead from di Grassi on the third lap but by lap six it was Buemi who had hit the front    and the Swiss driver never looked back.    The    took the checkered flag a comfortable three seconds clear of Vergne to seal a third consecutive win    the first driver to achieve the feat in Formula E    and his ninth overall in the   race series.  READ: How virtual racing breeds   success, READ: How ’humble’ star landed F1’s hottest drive  Buemi won the   in Hong Kong last October and then repeated the feat in Marrakech the following month.  This latest win gives the Swiss a maximum 75 points and a   cushion over nearest rival di Grassi.   But with nine races to go, Buemi is determined not to take his foot off the gas.  ”We need to build up some points . .. these things never last for ever,” Buemi said.  ”My team did a great job    let’s hope it continues for a few more races. At the end of the day I’m really happy.”  Di Grassi, who pushed Buemi all the way in last year’s championship, ended up claiming third to notch a 15th podium in 24 career starts in Formula E.   Vergne scored his first podium of the season with Buemi’s teammate Nico Prost coming home fourth followed by NextEV’s Nelson Piquet Jr. DS Virgin Racing’s Jose Maria Lopez, the only Argentine driver in the field, finished 10th to secure a point but there was disappointment for teammate Sam Bird. The Briton was defending the title he won last year but limped home last after suffering damage to the left rear of his car early on in the race.  Buenos Aires ePrix top 10 finishers:  1. Sebastien Buemi  2.   Vergne  3. Lucas di Grassi  4. Nico Prost  5. Nelson Piquet Jr.  6. Loic Duval  7. Daniel Abt  8. Jerome D’Ambrosio  9. Oliver Turvey  10. Jose Maria Lopez  You can watch highlights of the Buenos Aires ePrix on Supercharged’s February show    click this link to go to our motorsport page where you will find show times.   Round four of the   Formula E World Championship takes place in Mexico on April 1.  '"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['para'][56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buemi'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['subject_nouns'][56]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 2 - Extracting subject matter (second step of chunker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we can see that Subjects have been correctly identified. Now, let us extract the verb and object, combining them with object and verb to obtain subject matter of the paragraph. Before we proceed any further, we would need to train trigram tagger, let us use the nltk corpus for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = nltk.corpus.brown.tagged_sents()\n",
    "train_sents += nltk.corpus.conll2000.tagged_sents()\n",
    "train_sents += nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = DefaultTagger('NN')\n",
    "t1 = UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = BigramTagger(train_sents, backoff=t1)\n",
    "tagger = TrigramTagger(train_sents, backoff=t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_multi_word_subject(sentences, subject):\n",
    "\n",
    "    if len(subject.split()) == 1:\n",
    "        return sentences\n",
    "    subject_lst = subject.split()\n",
    "    sentences_lower = [[word.lower() for word in sentence]\n",
    "                        for sentence in sentences]\n",
    "    for i, sent in enumerate(sentences_lower):\n",
    "        if subject_lst[0] in sent:\n",
    "            for j, token in enumerate(sent):\n",
    "                start = subject_lst[0] == token\n",
    "                exists = subject_lst == sent[j:j+len(subject_lst)]\n",
    "                if start and exists:\n",
    "                    del sentences[i][j+1:j+len(subject_lst)]\n",
    "                    sentences[i][j] = subject\n",
    "    return sentences\n",
    "\n",
    "def tag_sentences(subject, paragraph):\n",
    "    \"\"\"Returns tagged sentences using POS tagging\"\"\"\n",
    "    trigram_tagger = tagger\n",
    "\n",
    "    # Tokenize Sentences and words\n",
    "    sentences = tokenize_sentences(paragraph)\n",
    "    merge_multi_word_subject(sentences, subject)\n",
    "\n",
    "    # Filter out sentences where subject is not present\n",
    "    sentences = [sentence for sentence in sentences if subject in\n",
    "                [word.lower() for word in sentence]]\n",
    "\n",
    "    # Tag each sentence\n",
    "    tagged_sents = [trigram_tagger.tag(sent) for sent in sentences]\n",
    "    return tagged_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nouns that being identified are seperated for example, Donald is tagged as one Noun and Trump as another, but they occus together, so the above function will help us in mergeing multi noun words into single token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extract Subject Action Object (subject matter)\n",
    "\n",
    "Based on the subject determined earlier, the action is identified with particular related to the subject using Verbs. The noun coming after verb is considered as object. Now, combining all the three gives us the subject matter we have been looking for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svo(sentence, subject):\n",
    "    subject_idx = next((i for i, v in enumerate(sentence)\n",
    "                    if v[0].lower() == subject), None)\n",
    "    data = {'subject': subject}\n",
    "    for i in range(subject_idx, len(sentence)):\n",
    "        found_action = False\n",
    "        for j, (token, tag) in enumerate(sentence[i+1:]):\n",
    "            if tag in VERBS:\n",
    "                data['action'] = token\n",
    "                found_action = True\n",
    "            if tag in NOUNS and found_action == True:\n",
    "                data['object'] = token\n",
    "                data['phrase'] = sentence[i: i+j+2]\n",
    "                return data\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_out(document):\n",
    "    document = clean_document(document)\n",
    "    subject = extract_subject(document)\n",
    "    tagged_sents = tag_sentences(subject, document)\n",
    "    svos = [get_svo(sentence, subject) for sentence in tagged_sents]\n",
    "    return svos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_all(x):\n",
    "    try:\n",
    "        return final_out(x)\n",
    "    except:\n",
    "        return 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Subject_Verb_Object']=data['para'].apply(for_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Idea2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different phrases generated can be found below for a sample set of articles. The results for the overall have been saved in the output_sao csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (CNN) Another ePrix, another victory for Sebastien Buemi.  The Renault eDams driver made it three wins out of three for the   season at the Buenos Aires ePrix with another impressive drive at the Puerto Madero Street Circuit on Saturday.  The reigning world champion led for the majority of the   race after starting third on the grid behind pole sitter Lucas di Grassi    a first for the ABT Schaeffler Audi Sport driver    and Techeetah’s   Vergne.  Vergne seized the lead from di Grassi on the third lap but by lap six it was Buemi who had hit the front    and the Swiss driver never looked back.    The    took the checkered flag a comfortable three seconds clear of Vergne to seal a third consecutive win    the first driver to achieve the feat in Formula E    and his ninth overall in the   race series.  READ: How virtual racing breeds   success, READ: How ’humble’ star landed F1’s hottest drive  Buemi won the   in Hong Kong last October and then repeated the feat in Marrakech the following month.  This latest win gives the Swiss a maximum 75 points and a   cushion over nearest rival di Grassi.   But with nine races to go, Buemi is determined not to take his foot off the gas.  ”We need to build up some points . .. these things never last for ever,” Buemi said.  ”My team did a great job    let’s hope it continues for a few more races. At the end of the day I’m really happy.”  Di Grassi, who pushed Buemi all the way in last year’s championship, ended up claiming third to notch a 15th podium in 24 career starts in Formula E.   Vergne scored his first podium of the season with Buemi’s teammate Nico Prost coming home fourth followed by NextEV’s Nelson Piquet Jr. DS Virgin Racing’s Jose Maria Lopez, the only Argentine driver in the field, finished 10th to secure a point but there was disappointment for teammate Sam Bird. The Briton was defending the title he won last year but limped home last after suffering damage to the left rear of his car early on in the race.  Buenos Aires ePrix top 10 finishers:  1. Sebastien Buemi  2.   Vergne  3. Lucas di Grassi  4. Nico Prost  5. Nelson Piquet Jr.  6. Loic Duval  7. Daniel Abt  8. Jerome D’Ambrosio  9. Oliver Turvey  10. Jose Maria Lopez  You can watch highlights of the Buenos Aires ePrix on Supercharged’s February show    click this link to go to our motorsport page where you will find show times.   Round four of the   Formula E World Championship takes place in Mexico on April 1.  '"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['para'][56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Buemi', 'NN'), ('hit', 'VBD'), ('front', 'NN')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][56][1]['phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Buemi', 'NN'),\n",
       " ('Hong', 'NNP'),\n",
       " ('Kong', 'NNP'),\n",
       " ('last', 'JJ'),\n",
       " ('October', 'NNP'),\n",
       " ('repeated', 'VBD'),\n",
       " ('feat', 'NN')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][56][2]['phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Buemi', 'NN'), ('determined', 'VBN'), ('take', 'VB'), ('foot', 'NN')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][56][3]['phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Buemi determined take foot' seems to be good title suggestion for the paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Litigation $50 million (originally $25 million) lawsuit by John Burris against BART on behalf of Grant's mother and daughter was settled for $2.8 million; Grant's father's lawsuit was denied\\nOscar Grant III was a 22-year-old African-American man who was fatally shot in the early morning hours of New Year's Day 2009 by BART Police Officer Johannes Mehserle in Oakland, California. Responding to reports of a fight on a crowded Bay Area Rapid Transit train returning from San Francisco, BART Police officers detained Grant and several other passengers on the platform at the Fruitvale BART Station. Two officers, including Mehserle, forced the unarmed Grant to lie face down on the platform. Mehserle drew his pistol and shot Grant in the back. Grant was rushed to Highland Hospital in Oakland and pronounced dead later that day. The events were captured on multiple official and private digital video and privately owned cell phone cameras. Owners disseminated their footage to media outlets and to various websites where it became viral. Both peaceful and violent protests of police actions took place in the following days.\""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['para'][27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grant', 'NP'),\n",
       " ('father', 'NN'),\n",
       " ('lawsuit', 'NN'),\n",
       " ('denied', 'VBN'),\n",
       " ('Oscar', 'NP'),\n",
       " ('Grant', 'NP'),\n",
       " ('III', 'NNP')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][27][1]['phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grant', 'NP'), ('lie', 'VB'), ('face', 'NN')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][27][3]['phrase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Grant', 'VB'), ('rushed', 'VBN'), ('Highland', 'NNP')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Subject_Verb_Object'][27][5]['phrase']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both, 'Grant father lawsuit denied' and 'Grant rushed Highland' both looks like good titles for the article at hand. So far, we have tried to develop a methodology based on our understanding to determine the title. However, let us see if any supervised learning model such a deep neural networks can be applied here to predict titles for the paragraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of  Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Networks are powerful models that have achieved excellent results recently in many different task. While Convoluted Neural Networks is good at solving most of the image related problems, Recurrent Neural Network could be a good choice for the problems involving sequence like current problem at hand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "For instance, language as we saw earlier- the sequence of words define their meaning. We are trying to use such data for any reasonable output, we need a network which has access to some prior knowledge about the data to completely understand it. The architecture of Recurrent Neural Networks take into account the requirement of prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks is our go-to architecture, for any sequence modelling problems. But there are many types of sequence related problems possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/types.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the input as well as the output are a sequence, this kind of preblem is called sequence to sequence models. And, it is widely used in Machine Translation and Video Captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture and Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence to sequence model has two parts – an encoder and a decoder. Both of them are basically two independant Neural Networks models combined. The encoder generates a one output for complete input and is passed into the decoder network to start predicting a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/seq.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider training the Sequence to Sequence deep neural model to predict the title for the passage passed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "Fake or real article dataset is considered to train the model, which is almost similar to the one dataset used earlier. To train a deep model with high accuracy we will need an substantial amount of data. \n",
    "\n",
    "The model training and codes are present in the different notebook - 'Extension - Sequence to Sequence Model using Glove'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the model are as described below : -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Maximum Input Sequence Length = 500\n",
    "    Maximum Target Sequence Length = 50\n",
    "    Maximum Input Vocabulary Size= 8000\n",
    "    Maximum Target Vocabulary Size = 2000\n",
    "\n",
    "    Dimension for the Glove Word Embeddings = 100\n",
    "\n",
    "    Training Details : \n",
    "        Hidden layers for each Encode and Decoder Model = 100\n",
    "        Batch Size = 16 \n",
    "        Epochs = 30 (as high as 150 were trained)\n",
    "        Optimizer = rmsprop\n",
    "        Activation = Softmax\n",
    "        Loss = Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of the model is below :- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/summ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/train.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at some of the generated headlines :- \n",
    "\n",
    "    Generated Headline:  california today: today: the u.s. in the the world\n",
    "    Original Headline:  Bill Paxton, Star of ‘Big Love’ and Movie Blockbusters, Dies at 61\n",
    "    \n",
    "    Generated Headline:  north and china is killed by the watch: china\n",
    "    Original Headline:  Oakland Fire Victims Included Performers, Educators and Lawyers\n",
    "    \n",
    "    Generated Headline:  china says u.s. american american have\n",
    "    Original Headline:  ‘Hamilton’ Inc.: The Path to a Billion-Dollar Broadway Show\n",
    "    \n",
    "    Generated Headline:  a new york times with a new york times\n",
    "    Original Headline:  Mute and Alone, He Was Never Short of Kind Words or Friends\n",
    "    \n",
    "\n",
    "Most of these dont seem to make sense for the model predicted. However, the model has learnt to predict to sequence of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the limited data and resources, the sequence to sequence model was not able to perform as expected, with accuracy of less than 10%. Adding of additional data led to memory error. Majority of the trainings led to prediction of single output for all the inputs due to not sufficient epochs or data. Irrespective of trying various combinations of the hyperparameters the model was not able to perform as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results looks promising in the paper - 'Generating News Headlines with Recurrent Neural Networks' by Konstantin Lopyrev by addition of attention layer in the encoder - decoder model. Training the model with attention layer and increasing data size can help us in achieving better results. This can be one further development to the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Idea 1 and Idea 2 : - \n",
    "\n",
    "https://streamhacker.com/2009/02/23/chunk-extraction-with-nltk/\n",
    "\n",
    "https://www.kaggle.com/snapcrack/all-the-news\n",
    "\n",
    "https://medium.com/@acrosson/extract-subject-matter-of-documents-using-nlp-e284c1c61824\n",
    "\n",
    "http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\n",
    "\n",
    "For Deep learning model :-\n",
    "\n",
    "https://nlp.stanford.edu/courses/cs224n/2015/reports/1.pdf\n",
    "\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/\n",
    "\n",
    "https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "\n",
    "https://github.com/chen0040/keras-text-summarization\n",
    "\n",
    "https://nbviewer.jupyter.org/github/hamelsmu/Seq2Seq_Tutorial/blob/master/notebooks/Tutorial.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
